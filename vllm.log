[0;36m(APIServer pid=536920)[0;0m INFO 01-19 22:25:49 [api_server.py:1351] vLLM API server version 0.13.0
[0;36m(APIServer pid=536920)[0;0m INFO 01-19 22:25:49 [utils.py:253] non-default args: {'host': '0.0.0.0', 'model': 'Qwen/Qwen2.5-Coder-14B-Instruct', 'trust_remote_code': True, 'dtype': 'bfloat16', 'max_model_len': 24576, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'max_num_seqs': 8}
[0;36m(APIServer pid=536920)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[0;36m(APIServer pid=536920)[0;0m INFO 01-19 22:25:50 [model.py:514] Resolved architecture: Qwen2ForCausalLM
[0;36m(APIServer pid=536920)[0;0m INFO 01-19 22:25:50 [model.py:1661] Using max model len 24576
[0;36m(APIServer pid=536920)[0;0m INFO 01-19 22:25:50 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(APIServer pid=536920)[0;0m WARNING 01-19 22:25:50 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
[0;36m(APIServer pid=536920)[0;0m INFO 01-19 22:25:50 [vllm.py:722] Cudagraph is disabled under eager mode
[0;36m(EngineCore_DP0 pid=537018)[0;0m INFO 01-19 22:25:57 [core.py:93] Initializing a V1 LLM engine (v0.13.0) with config: model='Qwen/Qwen2.5-Coder-14B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-Coder-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=24576, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=Qwen/Qwen2.5-Coder-14B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 0, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=537018)[0;0m INFO 01-19 22:25:57 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.21.35.234:36847 backend=nccl
[0;36m(EngineCore_DP0 pid=537018)[0;0m INFO 01-19 22:25:57 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=537018)[0;0m INFO 01-19 22:25:58 [gpu_model_runner.py:3562] Starting to load model Qwen/Qwen2.5-Coder-14B-Instruct...
[0;36m(EngineCore_DP0 pid=537018)[0;0m INFO 01-19 22:26:13 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(EngineCore_DP0 pid=537018)[0;0m ERROR 01-19 22:26:14 [gpu_model_runner.py:3657] Failed to load model - not enough GPU memory. Try lowering --gpu-memory-utilization to free memory for weights, increasing --tensor-parallel-size, or using --quantization. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more tips. (original error: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 0 has a total capacity of 15.47 GiB of which 115.31 MiB is free. Including non-PyTorch memory, this process has 15.34 GiB memory in use. Of the allocated memory 14.95 GiB is allocated by PyTorch, and 12.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables))
[0;36m(EngineCore_DP0 pid=537018)[0;0m ERROR 01-19 22:26:14 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=537018)[0;0m ERROR 01-19 22:26:14 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=537018)[0;0m ERROR 01-19 22:26:14 [core.py:866]   File "/home/k/.local/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=537018)[0;0m ERROR 01-19 22:26:14 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=537018)[0;0m ERROR 01-19 22:26:14 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=537018)[0;0m ERROR 01-19 22:26:14 [core.py:866]   File "/home/k/.local/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=537018)[0;0m ERROR 01-19 22:26:14 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=537018)[0;0m ERROR 01-19 22:26:14 [core.py:866]   File "/home/k/.local/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[0;36m(EngineCore_DP0 pid=537018)[0;0m ERROR 01-19 22:26:14 [core.py:866]     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=537018)[0;0m ERROR 01-19 22:26:14 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=537018)[0;0m ERROR 01-19 22:26:14 [core.py:866]   File "/home/k/.local/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=537018)[0;0m ERROR 01-19 22:26:14 [core.py:866]     self._init_executor()
[0;36m(EngineCore_DP0 pid=537018)[0;0m ERROR 01-19 22:26:14 [core.py:866]   File "/home/k/.local/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[0;36m(EngineCore_DP0 pid=537018)[0;0m ERROR 01-19 22:26:14 [core.py:866]     self.driver_worker.load_model()
[0;36m(EngineCore_DP0 pid=537018)[0;0m ERROR 01-19 22:26:14 [core.py:866]   File "/home/k/.local/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 289, in load_model
[0;36m(EngineCore_DP0 pid=537018)[0;0m ERROR 01-19 22:26:14 [core.py:866]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[0;36m(EngineCore_DP0 pid=537018)[0;0m ERROR 01-19 22:26:14 [core.py:866]   File "/home/k/.local/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3658, in load_model
[0;36m(EngineCore_DP0 pid=537018)[0;0m ERROR 01-19 22:26:14 [core.py:866]     raise e
[0;36m(EngineCore_DP0 pid=537018)[0;0m ERROR 01-19 22:26:14 [core.py:866]   File "/home/k/.local/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3581, in load_model
[0;36m(EngineCore_DP0 pid=537018)[0;0m ERROR 01-19 22:26:14 [core.py:866]     self.model = model_loader.load_model(
[0;36m(EngineCore_DP0 pid=537018)[0;0m ERROR 01-19 22:26:14 [core.py:866]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=537018)[0;0m ERROR 01-19 22:26:14 [core.py:866]   File "/home/k/.local/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[0;36m(EngineCore_DP0 pid=537018)[0;0m ERROR 01-19 22:26:14 [core.py:866]     model = initialize_model(
[0;36m(EngineCore_DP0 pid=537018)[0;0m ERROR 01-19 22:26:14 [core.py:866]             ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=537018)[0;0m ERROR 01-19 22:26:14 [core.py:866]   File "/home/k/.local/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
[0;36m(EngineCore_DP0 pid=537018)[0;0m ERROR 01-19 22:26:14 [core.py:866]     return model_class(vllm_config=vllm_config, prefix=prefix)
[0;36m(EngineCore_DP0 pid=537018)[0;0m ERROR 01-19 22:26:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=537018)[0;0m ERROR 01-19 22:26:14 [core.py:866]   File "/home/k/.local/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 543, in __init__
[0;36m(EngineCore_DP0 pid=537018)[0;0m ERROR 01-19 22:26:14 [core.py:866]     self.model = Qwen2Model(
[0;36m(EngineCore_DP0 pid=537018)[0;0m ERROR 01-19 22:26:14 [core.py:866]                  ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=537018)[0;0m ERROR 01-19 22:26:14 [core.py:866]   File "/home/k/.local/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 291, in __init__
[0;36m(EngineCore_DP0 pid=537018)[0;0m ERROR 01-19 22:26:14 [core.py:866]     old_init(self, **kwargs)
[0;36m(EngineCore_DP0 pid=537018)[0;0m ERROR 01-19 22:26:14 [core.py:866]   File "/home/k/.local/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 394, in __init__
[0;36m(EngineCore_DP0 pid=537018)[0;0m ERROR 01-19 22:26:14 [core.py:866]     self.start_layer, self.end_layer, self.layers = make_layers(
[0;36m(EngineCore_DP0 pid=537018)[0;0m ERROR 01-19 22:26:14 [core.py:866]                                                     ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=537018)[0;0m ERROR 01-19 22:26:14 [core.py:866]   File "/home/k/.local/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 606, in make_layers
[0;36m(EngineCore_DP0 pid=537018)[0;0m ERROR 01-19 22:26:14 [core.py:866]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[0;36m(EngineCore_DP0 pid=537018)[0;0m ERROR 01-19 22:26:14 [core.py:866]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=537018)[0;0m ERROR 01-19 22:26:14 [core.py:866]   File "/home/k/.local/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 396, in <lambda>
[0;36m(EngineCore_DP0 pid=537018)[0;0m ERROR 01-19 22:26:14 [core.py:866]     lambda prefix: decoder_layer_type(
[0;36m(EngineCore_DP0 pid=537018)[0;0m ERROR 01-19 22:26:14 [core.py:866]                    ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=537018)[0;0m ERROR 01-19 22:26:14 [core.py:866]   File "/home/k/.local/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 272, in __init__
[0;36m(EngineCore_DP0 pid=537018)[0;0m ERROR 01-19 22:26:14 [core.py:866]     self.mlp = Qwen2MLP(
[0;36m(EngineCore_DP0 pid=537018)[0;0m ERROR 01-19 22:26:14 [core.py:866]                ^^^^^^^^^
[0;36m(EngineCore_DP0 pid=537018)[0;0m ERROR 01-19 22:26:14 [core.py:866]   File "/home/k/.local/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 85, in __init__
[0;36m(EngineCore_DP0 pid=537018)[0;0m ERROR 01-19 22:26:14 [core.py:866]     self.gate_up_proj = MergedColumnParallelLinear(
[0;36m(EngineCore_DP0 pid=537018)[0;0m ERROR 01-19 22:26:14 [core.py:866]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=537018)[0;0m ERROR 01-19 22:26:14 [core.py:866]   File "/home/k/.local/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[0;36m(EngineCore_DP0 pid=537018)[0;0m ERROR 01-19 22:26:14 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=537018)[0;0m ERROR 01-19 22:26:14 [core.py:866]   File "/home/k/.local/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[0;36m(EngineCore_DP0 pid=537018)[0;0m ERROR 01-19 22:26:14 [core.py:866]     self.quant_method.create_weights(
[0;36m(EngineCore_DP0 pid=537018)[0;0m ERROR 01-19 22:26:14 [core.py:866]   File "/home/k/.local/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[0;36m(EngineCore_DP0 pid=537018)[0;0m ERROR 01-19 22:26:14 [core.py:866]     data=torch.empty(
[0;36m(EngineCore_DP0 pid=537018)[0;0m ERROR 01-19 22:26:14 [core.py:866]          ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=537018)[0;0m ERROR 01-19 22:26:14 [core.py:866]   File "/home/k/.local/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[0;36m(EngineCore_DP0 pid=537018)[0;0m ERROR 01-19 22:26:14 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=537018)[0;0m ERROR 01-19 22:26:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=537018)[0;0m ERROR 01-19 22:26:14 [core.py:866] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 0 has a total capacity of 15.47 GiB of which 115.31 MiB is free. Including non-PyTorch memory, this process has 15.34 GiB memory in use. Of the allocated memory 14.95 GiB is allocated by PyTorch, and 12.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[0;36m(EngineCore_DP0 pid=537018)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=537018)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=537018)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=537018)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=537018)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=537018)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=537018)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=537018)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=537018)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=537018)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=537018)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=537018)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=537018)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=537018)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[0;36m(EngineCore_DP0 pid=537018)[0;0m     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=537018)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=537018)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=537018)[0;0m     self._init_executor()
[0;36m(EngineCore_DP0 pid=537018)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[0;36m(EngineCore_DP0 pid=537018)[0;0m     self.driver_worker.load_model()
[0;36m(EngineCore_DP0 pid=537018)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 289, in load_model
[0;36m(EngineCore_DP0 pid=537018)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[0;36m(EngineCore_DP0 pid=537018)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3658, in load_model
[0;36m(EngineCore_DP0 pid=537018)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=537018)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3581, in load_model
[0;36m(EngineCore_DP0 pid=537018)[0;0m     self.model = model_loader.load_model(
[0;36m(EngineCore_DP0 pid=537018)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=537018)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
[0;36m(EngineCore_DP0 pid=537018)[0;0m     model = initialize_model(
[0;36m(EngineCore_DP0 pid=537018)[0;0m             ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=537018)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
[0;36m(EngineCore_DP0 pid=537018)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[0;36m(EngineCore_DP0 pid=537018)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=537018)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 543, in __init__
[0;36m(EngineCore_DP0 pid=537018)[0;0m     self.model = Qwen2Model(
[0;36m(EngineCore_DP0 pid=537018)[0;0m                  ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=537018)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 291, in __init__
[0;36m(EngineCore_DP0 pid=537018)[0;0m     old_init(self, **kwargs)
[0;36m(EngineCore_DP0 pid=537018)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 394, in __init__
[0;36m(EngineCore_DP0 pid=537018)[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(
[0;36m(EngineCore_DP0 pid=537018)[0;0m                                                     ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=537018)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 606, in make_layers
[0;36m(EngineCore_DP0 pid=537018)[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[0;36m(EngineCore_DP0 pid=537018)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=537018)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 396, in <lambda>
[0;36m(EngineCore_DP0 pid=537018)[0;0m     lambda prefix: decoder_layer_type(
[0;36m(EngineCore_DP0 pid=537018)[0;0m                    ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=537018)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 272, in __init__
[0;36m(EngineCore_DP0 pid=537018)[0;0m     self.mlp = Qwen2MLP(
[0;36m(EngineCore_DP0 pid=537018)[0;0m                ^^^^^^^^^
[0;36m(EngineCore_DP0 pid=537018)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 85, in __init__
[0;36m(EngineCore_DP0 pid=537018)[0;0m     self.gate_up_proj = MergedColumnParallelLinear(
[0;36m(EngineCore_DP0 pid=537018)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=537018)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 631, in __init__
[0;36m(EngineCore_DP0 pid=537018)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=537018)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 484, in __init__
[0;36m(EngineCore_DP0 pid=537018)[0;0m     self.quant_method.create_weights(
[0;36m(EngineCore_DP0 pid=537018)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 215, in create_weights
[0;36m(EngineCore_DP0 pid=537018)[0;0m     data=torch.empty(
[0;36m(EngineCore_DP0 pid=537018)[0;0m          ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=537018)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[0;36m(EngineCore_DP0 pid=537018)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=537018)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=537018)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 0 has a total capacity of 15.47 GiB of which 115.31 MiB is free. Including non-PyTorch memory, this process has 15.34 GiB memory in use. Of the allocated memory 14.95 GiB is allocated by PyTorch, and 12.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W119 22:26:14.134276848 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[0;36m(APIServer pid=536920)[0;0m Traceback (most recent call last):
[0;36m(APIServer pid=536920)[0;0m   File "<frozen runpy>", line 198, in _run_module_as_main
[0;36m(APIServer pid=536920)[0;0m   File "<frozen runpy>", line 88, in _run_code
[0;36m(APIServer pid=536920)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 1469, in <module>
[0;36m(APIServer pid=536920)[0;0m     uvloop.run(run_server(args))
[0;36m(APIServer pid=536920)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/uvloop/__init__.py", line 109, in run
[0;36m(APIServer pid=536920)[0;0m     return __asyncio.run(
[0;36m(APIServer pid=536920)[0;0m            ^^^^^^^^^^^^^^
[0;36m(APIServer pid=536920)[0;0m   File "/usr/lib/python3.12/asyncio/runners.py", line 194, in run
[0;36m(APIServer pid=536920)[0;0m     return runner.run(main)
[0;36m(APIServer pid=536920)[0;0m            ^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=536920)[0;0m   File "/usr/lib/python3.12/asyncio/runners.py", line 118, in run
[0;36m(APIServer pid=536920)[0;0m     return self._loop.run_until_complete(task)
[0;36m(APIServer pid=536920)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=536920)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[0;36m(APIServer pid=536920)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/uvloop/__init__.py", line 61, in wrapper
[0;36m(APIServer pid=536920)[0;0m     return await main
[0;36m(APIServer pid=536920)[0;0m            ^^^^^^^^^^
[0;36m(APIServer pid=536920)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 1398, in run_server
[0;36m(APIServer pid=536920)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[0;36m(APIServer pid=536920)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 1417, in run_server_worker
[0;36m(APIServer pid=536920)[0;0m     async with build_async_engine_client(
[0;36m(APIServer pid=536920)[0;0m   File "/usr/lib/python3.12/contextlib.py", line 210, in __aenter__
[0;36m(APIServer pid=536920)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=536920)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=536920)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 172, in build_async_engine_client
[0;36m(APIServer pid=536920)[0;0m     async with build_async_engine_client_from_engine_args(
[0;36m(APIServer pid=536920)[0;0m   File "/usr/lib/python3.12/contextlib.py", line 210, in __aenter__
[0;36m(APIServer pid=536920)[0;0m     return await anext(self.gen)
[0;36m(APIServer pid=536920)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=536920)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 213, in build_async_engine_client_from_engine_args
[0;36m(APIServer pid=536920)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[0;36m(APIServer pid=536920)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=536920)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 215, in from_vllm_config
[0;36m(APIServer pid=536920)[0;0m     return cls(
[0;36m(APIServer pid=536920)[0;0m            ^^^^
[0;36m(APIServer pid=536920)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 134, in __init__
[0;36m(APIServer pid=536920)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[0;36m(APIServer pid=536920)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=536920)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 121, in make_async_mp_client
[0;36m(APIServer pid=536920)[0;0m     return AsyncMPClient(*client_args)
[0;36m(APIServer pid=536920)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=536920)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 820, in __init__
[0;36m(APIServer pid=536920)[0;0m     super().__init__(
[0;36m(APIServer pid=536920)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 477, in __init__
[0;36m(APIServer pid=536920)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[0;36m(APIServer pid=536920)[0;0m   File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
[0;36m(APIServer pid=536920)[0;0m     next(self.gen)
[0;36m(APIServer pid=536920)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 903, in launch_core_engines
[0;36m(APIServer pid=536920)[0;0m     wait_for_engine_startup(
[0;36m(APIServer pid=536920)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
[0;36m(APIServer pid=536920)[0;0m     raise RuntimeError(
[0;36m(APIServer pid=536920)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
