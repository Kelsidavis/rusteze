[0;36m(APIServer pid=558456)[0;0m INFO 01-19 23:40:18 [api_server.py:1351] vLLM API server version 0.13.0
[0;36m(APIServer pid=558456)[0;0m INFO 01-19 23:40:18 [utils.py:253] non-default args: {'host': '0.0.0.0', 'model': 'Qwen/Qwen2.5-Coder-7B-Instruct', 'trust_remote_code': True, 'dtype': 'bfloat16', 'max_model_len': 32768, 'gpu_memory_utilization': 0.7, 'max_num_batched_tokens': 4096, 'max_num_seqs': 32, 'enable_chunked_prefill': True}
[0;36m(APIServer pid=558456)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[0;36m(APIServer pid=558456)[0;0m INFO 01-19 23:40:19 [model.py:514] Resolved architecture: Qwen2ForCausalLM
[0;36m(APIServer pid=558456)[0;0m INFO 01-19 23:40:19 [model.py:1661] Using max model len 32768
[0;36m(APIServer pid=558456)[0;0m INFO 01-19 23:40:19 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=4096.
[0;36m(EngineCore_DP0 pid=558551)[0;0m INFO 01-19 23:40:26 [core.py:93] Initializing a V1 LLM engine (v0.13.0) with config: model='Qwen/Qwen2.5-Coder-7B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-Coder-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=Qwen/Qwen2.5-Coder-7B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [4096], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 64, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=558551)[0;0m INFO 01-19 23:40:26 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.21.35.234:41749 backend=nccl
[0;36m(EngineCore_DP0 pid=558551)[0;0m INFO 01-19 23:40:26 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=558551)[0;0m INFO 01-19 23:40:27 [gpu_model_runner.py:3562] Starting to load model Qwen/Qwen2.5-Coder-7B-Instruct...
[0;36m(EngineCore_DP0 pid=558551)[0;0m INFO 01-19 23:40:27 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(EngineCore_DP0 pid=558551)[0;0m INFO 01-19 23:55:05 [weight_utils.py:487] Time spent downloading weights for Qwen/Qwen2.5-Coder-7B-Instruct: 877.584534 seconds
[0;36m(EngineCore_DP0 pid=558551)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=558551)[0;0m Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.94it/s]
[0;36m(EngineCore_DP0 pid=558551)[0;0m Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.62it/s]
[0;36m(EngineCore_DP0 pid=558551)[0;0m Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.52it/s]
[0;36m(EngineCore_DP0 pid=558551)[0;0m Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  2.00it/s]
[0;36m(EngineCore_DP0 pid=558551)[0;0m Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.85it/s]
[0;36m(EngineCore_DP0 pid=558551)[0;0m 
[0;36m(EngineCore_DP0 pid=558551)[0;0m INFO 01-19 23:55:08 [default_loader.py:308] Loading weights took 2.20 seconds
[0;36m(EngineCore_DP0 pid=558551)[0;0m INFO 01-19 23:55:08 [gpu_model_runner.py:3659] Model loading took 14.1931 GiB memory and 880.678751 seconds
[0;36m(EngineCore_DP0 pid=558551)[0;0m INFO 01-19 23:55:12 [backends.py:643] Using cache directory: /home/k/.cache/vllm/torch_compile_cache/708c1b2aab/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=558551)[0;0m INFO 01-19 23:55:12 [backends.py:703] Dynamo bytecode transform time: 3.66 s
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]   File "/home/k/.local/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]   File "/home/k/.local/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]   File "/home/k/.local/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]   File "/home/k/.local/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]   File "/home/k/.local/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]   File "/home/k/.local/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]   File "/home/k/.local/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]   File "/home/k/.local/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]   File "/home/k/.local/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]   File "/home/k/.local/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]   File "/home/k/.local/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]   File "/home/k/.local/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]     outputs = self.model(
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]   File "/home/k/.local/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]   File "/home/k/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]   File "/home/k/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]   File "/home/k/.local/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]   File "/home/k/.local/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]   File "/home/k/.local/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]   File "/home/k/.local/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]   File "/home/k/.local/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]   File "/home/k/.local/lib/python3.12/site-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]   File "/home/k/.local/lib/python3.12/site-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]   File "/home/k/.local/lib/python3.12/site-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]   File "/home/k/.local/lib/python3.12/site-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]   File "/home/k/.local/lib/python3.12/site-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]   File "/home/k/.local/lib/python3.12/site-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]   File "/home/k/.local/lib/python3.12/site-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]   File "/home/k/.local/lib/python3.12/site-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]   File "/home/k/.local/lib/python3.12/site-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]   File "/home/k/.local/lib/python3.12/site-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866]     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=558551)[0;0m ERROR 01-19 23:55:19 [core.py:866] torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 1.02 GiB. GPU 0 has a total capacity of 15.47 GiB of which 857.31 MiB is free. Including non-PyTorch memory, this process has 14.61 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 19.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[0;36m(EngineCore_DP0 pid=558551)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=558551)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=558551)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=558551)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=558551)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=558551)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=558551)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 870, in run_engine_core
[0;36m(EngineCore_DP0 pid=558551)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=558551)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=558551)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=558551)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=558551)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=558551)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=558551)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=558551)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=558551)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=558551)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=558551)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=558551)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=558551)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=558551)[0;0m     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=558551)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=558551)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=558551)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=558551)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=558551)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP0 pid=558551)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=558551)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=558551)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=558551)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=558551)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=558551)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
[0;36m(EngineCore_DP0 pid=558551)[0;0m     self.model_runner.profile_run()
[0;36m(EngineCore_DP0 pid=558551)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
[0;36m(EngineCore_DP0 pid=558551)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[0;36m(EngineCore_DP0 pid=558551)[0;0m                                         ^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=558551)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=558551)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=558551)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=558551)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
[0;36m(EngineCore_DP0 pid=558551)[0;0m     outputs = self.model(
[0;36m(EngineCore_DP0 pid=558551)[0;0m               ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=558551)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 220, in __call__
[0;36m(EngineCore_DP0 pid=558551)[0;0m     return self.runnable(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=558551)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=558551)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[0;36m(EngineCore_DP0 pid=558551)[0;0m     return self._call_impl(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=558551)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=558551)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[0;36m(EngineCore_DP0 pid=558551)[0;0m     return forward_call(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=558551)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=558551)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 583, in forward
[0;36m(EngineCore_DP0 pid=558551)[0;0m     hidden_states = self.model(
[0;36m(EngineCore_DP0 pid=558551)[0;0m                     ^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=558551)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 526, in __call__
[0;36m(EngineCore_DP0 pid=558551)[0;0m     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
[0;36m(EngineCore_DP0 pid=558551)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=558551)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 218, in __call__
[0;36m(EngineCore_DP0 pid=558551)[0;0m     return self._call_with_optional_nvtx_range(
[0;36m(EngineCore_DP0 pid=558551)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=558551)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
[0;36m(EngineCore_DP0 pid=558551)[0;0m     return callable_fn(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=558551)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=558551)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
[0;36m(EngineCore_DP0 pid=558551)[0;0m     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[0;36m(EngineCore_DP0 pid=558551)[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=558551)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=558551)[0;0m     raise InductorError(e, currentframe()).with_traceback(
[0;36m(EngineCore_DP0 pid=558551)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
[0;36m(EngineCore_DP0 pid=558551)[0;0m     mb_compiled_graph = fx_codegen_and_compile(
[0;36m(EngineCore_DP0 pid=558551)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=558551)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
[0;36m(EngineCore_DP0 pid=558551)[0;0m     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
[0;36m(EngineCore_DP0 pid=558551)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=558551)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
[0;36m(EngineCore_DP0 pid=558551)[0;0m     compiled_module = graph.compile_to_module()
[0;36m(EngineCore_DP0 pid=558551)[0;0m                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=558551)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
[0;36m(EngineCore_DP0 pid=558551)[0;0m     return self._compile_to_module()
[0;36m(EngineCore_DP0 pid=558551)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=558551)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
[0;36m(EngineCore_DP0 pid=558551)[0;0m     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
[0;36m(EngineCore_DP0 pid=558551)[0;0m                                                              ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=558551)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/torch/_inductor/graph.py", line 2271, in codegen
[0;36m(EngineCore_DP0 pid=558551)[0;0m     result = self.wrapper_code.generate(self.is_inference)
[0;36m(EngineCore_DP0 pid=558551)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=558551)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
[0;36m(EngineCore_DP0 pid=558551)[0;0m     return self._generate(is_inference)
[0;36m(EngineCore_DP0 pid=558551)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=558551)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
[0;36m(EngineCore_DP0 pid=558551)[0;0m     self.generate_and_run_autotune_block()
[0;36m(EngineCore_DP0 pid=558551)[0;0m   File "/home/k/.local/lib/python3.12/site-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
[0;36m(EngineCore_DP0 pid=558551)[0;0m     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
[0;36m(EngineCore_DP0 pid=558551)[0;0m torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 1.02 GiB. GPU 0 has a total capacity of 15.47 GiB of which 857.31 MiB is free. Including non-PyTorch memory, this process has 14.61 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 19.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W119 23:55:20.994790197 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[W119 23:55:21.648310708 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
